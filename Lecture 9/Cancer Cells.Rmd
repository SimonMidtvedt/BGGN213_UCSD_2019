---
title: "Unsupervised Learning Analysis of Human Breast Cancer Cells"
author: "Simon Midtvedt"
date: "2/8/2019"
output: html_document
---

## 1. Preparing the Data
```{r}
# Save your input data file to a new 'data' directory
fna.data <- "data/WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data)

# Convert the features of the data: wisc.data
wisc.data <- as.matrix(wisc.df[,3:32])

# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id
#head(wisc.data)

# Create diagnosis vector by completing the missing code
diagnosis <- as.numeric(wisc.df[,2]=="M")
```

### Q1. How many observations are in this dataset
```{r}
dim(wisc.data)
```

### Q2. How many variables/features in the data are suffixed with _mean?
```{r}
length(grep("mean",colnames(wisc.data)))
```

### Q3. How many of the observations have a malignant diagnosis?
```{r}
sum(diagnosis)
```

## 2. Principal Component Analysis
```{r}
# Check column means and standard deviations
#colMeans(wisc.data)  
#apply(wisc.data,2,sd)

# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(scale(wisc.data))

# Look at summary of results
summary(wisc.pr)
```

### Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
- 44.3%

### Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
- 3

### Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
- 7

```{r}
biplot(wisc.pr)
```

### Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
- Hard to understand. Too much information and annotations.

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,c(1,2)], col = (diagnosis+1), 
     xlab = "PC1", ylab = "PC2")
```


### Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,c(1,3)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```
- Cleaner cut separating the two subgroups in PC1 vs. PC2 than in PC1 vs. PC3.

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
par(mfcol=c(1,2))
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")

# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
## ggplot based graph
#install.packages("factoextra")
#library(factoextra)
```

### Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation[,1]
```


### Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
- 5

## 3. Hierarchical Clustering

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled)

wisc.hclust <- hclust(data.dist, method = "complete")
```


### Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```

### Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?
```{r}
table(cutree(wisc.hclust,k=5), diagnosis)
```

## 4. K-means clustering
```{r}
wisc.km <- kmeans(scale(wisc.data), centers=2, nstart=20)
table(wisc.km$cluster, diagnosis)
```

### Q13. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
- Pretty good, I would say better than hclust.

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

## 5. Combining Methods
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method= "ward.D2")
plot(wisc.pr.hclust)
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```
```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis+1)
```
```{r}
#library(rgl)
#plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=diagnosis+1)
```







